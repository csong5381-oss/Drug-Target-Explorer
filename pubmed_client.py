import requests
import xml.etree.ElementTree as ET
from typing import List, Dict, Optional, Tuple
import time
import re
import pandas as pd
import os


class PubMedClient:
    def __init__(self, config: Dict, paths_config: Dict):
        self.base_url = config['pubmed']['base_url']
        self.email = config['pubmed']['email']
        self.tool = config['pubmed']['tool']
        self.max_results = config['pubmed']['max_results']
        self.rate_limit = config['pubmed']['rate_limit']
        self.paths_config = paths_config

        # è¯ç‰©ç±»å‹å…³é”®è¯åº“
        self.drug_type_keywords = {
            'hormone': ['thyroid', 'estrogen', 'testosterone', 'cortisol', 'insulin',
                        'levothyroxine', 'hormone', 'steroid', 'glucocorticoid', 'progesterone'],
            'antibody': ['mab', 'izumab', 'umab', 'ximab', 'antibody', 'monoclonal'],
            'inhibitor': ['inib', 'ostat', 'prazole', 'tide', 'artan', 'olol', 'vastatin'],
            'antibiotic': ['mycin', 'cillin', 'floxacin', 'cycline', 'azole', 'penem'],
            'antiviral': ['vir', 'ciclovir', 'navir', 'previr', 'buvir', 'mivir'],
            'chemotherapy': ['platin', 'taxel', 'rubicin', 'citabine', 'mustine'],
            'biologic': ['cept', 'ercept', 'imumab', 'alimumab', 'cept'],
            'vaccine': ['vaccine', 'vax', 'ccine']
        }

    def smart_search_drug_articles(self, drug_name: str, max_results: int = None) -> List[Dict]:
        """
        æ™ºèƒ½è¯ç‰©æ–‡çŒ®æœç´¢
        """
        if max_results is None:
            max_results = self.max_results

        print(f"ğŸ¯ å¯åŠ¨æ™ºèƒ½æœç´¢: {drug_name}")
        start_time = time.time()

        # 1. æ£€æµ‹è¯ç‰©ç±»å‹
        drug_type, confidence = self._detect_drug_type_with_confidence(drug_name)
        print(f"   ğŸ§¬ è¯ç‰©ç±»å‹æ£€æµ‹: {drug_type} (ç½®ä¿¡åº¦: {confidence:.1%})")

        # 2. ç”Ÿæˆä¼˜åŒ–çš„æœç´¢ç­–ç•¥
        search_strategies = self._generate_optimized_strategies(drug_name, drug_type, confidence)
        print(f"   ğŸ“‹ ç”Ÿæˆ {len(search_strategies)} ä¸ªæœç´¢ç­–ç•¥")

        # 3. æ‰§è¡Œæ™ºèƒ½æœç´¢
        all_article_ids = []
        successful_strategies = 0

        for i, (strategy_name, search_terms) in enumerate(search_strategies.items(), 1):
            print(f"   ğŸ¯ æ‰§è¡Œç­–ç•¥ {i}/{len(search_strategies)}: {strategy_name}")

            strategy_ids = self._execute_search_strategy_with_retry(
                drug_name, search_terms, max_results - len(all_article_ids)
            )

            if strategy_ids:
                new_ids = [id for id in strategy_ids if id not in all_article_ids]
                if new_ids:
                    all_article_ids.extend(new_ids)
                    successful_strategies += 1
                    print(f"   âœ… ç­–ç•¥æˆåŠŸ: æ‰¾åˆ° {len(new_ids)} ç¯‡æ–°æ–‡çŒ®")
                else:
                    print(f"   âš ï¸  ç­–ç•¥æœªæ‰¾åˆ°æ–°æ–‡çŒ®")
            else:
                print(f"   âŒ ç­–ç•¥å¤±è´¥")

            # å¦‚æœå·²è¾¾åˆ°ç›®æ ‡ï¼Œæå‰ç»“æŸ
            if len(set(all_article_ids)) >= max_results:
                print(f"   â¹ï¸  å·²è¾¾åˆ°ç›®æ ‡æ–‡çŒ®æ•° ({max_results})ï¼Œæå‰ç»“æŸæœç´¢")
                break

            # ç­–ç•¥é—´å»¶è¿Ÿ
            if i < len(search_strategies):
                time.sleep(0.8)

        # 4. è·å–æ–‡çŒ®è¯¦ç»†ä¿¡æ¯
        unique_ids = list(set(all_article_ids))[:max_results]
        print(f"   ğŸ“Š æœç´¢å®Œæˆ: {successful_strategies}/{len(search_strategies)} ä¸ªç­–ç•¥æˆåŠŸ")
        print(f"   ğŸ¯ æ€»è®¡æ‰¾åˆ° {len(unique_ids)} ç¯‡å”¯ä¸€æ–‡çŒ®")

        if not unique_ids:
            print(f"âŒ æœªæ‰¾åˆ°è¯ç‰© '{drug_name}' çš„ç›¸å…³æ–‡çŒ®")
            return []

        print(f"   ğŸ“„ è·å– {len(unique_ids)} ç¯‡æ–‡çŒ®çš„è¯¦ç»†ä¿¡æ¯...")
        articles = self.get_article_details(unique_ids)

        # 5. å¢å¼ºæ–‡ç« ä¿¡æ¯
        enhanced_articles = []
        for article in articles:
            # æ£€æµ‹è¯­è¨€
            article['language'] = self._detect_language(article.get('abstract', ''))
            # æ ‡è®°è¯ç‰©ç±»å‹
            article['drug_type'] = drug_type
            enhanced_articles.append(article)

        # 6. ç»Ÿè®¡ä¿¡æ¯
        elapsed_time = time.time() - start_time
        language_stats = {}
        for article in enhanced_articles:
            lang = article['language']
            language_stats[lang] = language_stats.get(lang, 0) + 1

        print(f"   ğŸŒ è¯­è¨€åˆ†å¸ƒ: {language_stats}")
        print(f"âœ… æ™ºèƒ½æœç´¢å®Œæˆ: {len(enhanced_articles)} ç¯‡æ–‡çŒ®ï¼Œè€—æ—¶ {elapsed_time:.1f} ç§’")

        return enhanced_articles

    def _detect_drug_type_with_confidence(self, drug_name: str) -> Tuple[str, float]:
        """æ£€æµ‹è¯ç‰©ç±»å‹å¹¶è¿”å›ç½®ä¿¡åº¦"""
        drug_lower = drug_name.lower()

        # æ£€æŸ¥ç²¾ç¡®åŒ¹é…
        for drug_type, keywords in self.drug_type_keywords.items():
            for keyword in keywords:
                if keyword in drug_lower or drug_lower in keyword:
                    # è®¡ç®—ç½®ä¿¡åº¦ï¼šå®Œå…¨åŒ¹é… > éƒ¨åˆ†åŒ¹é…
                    if drug_lower == keyword:
                        return drug_type, 0.95
                    elif keyword in drug_lower:
                        return drug_type, 0.8
                    else:
                        return drug_type, 0.7

        # æ£€æŸ¥åç¼€æ¨¡å¼
        suffix_patterns = {
            'mab': ('antibody', 0.9),
            'inib': ('inhibitor', 0.85),
            'vir': ('antiviral', 0.8),
            'mycin': ('antibiotic', 0.85),
            'cillin': ('antibiotic', 0.85),
            'oxacin': ('antibiotic', 0.8),
            'vastatin': ('inhibitor', 0.8),
            'prazole': ('inhibitor', 0.75),
            'artan': ('inhibitor', 0.7),
            'olol': ('inhibitor', 0.7),
            'thasone': ('hormone', 0.8),
            'thyrine': ('hormone', 0.75)
        }

        for suffix, (drug_type, confidence) in suffix_patterns.items():
            if drug_lower.endswith(suffix):
                return drug_type, confidence

        # é»˜è®¤ç±»å‹
        return 'general', 0.5

    def _generate_optimized_strategies(self, drug_name: str, drug_type: str, confidence: float) -> Dict[str, List[str]]:
        """ç”Ÿæˆä¼˜åŒ–çš„æœç´¢ç­–ç•¥"""
        strategies = {}

        # åŸºç¡€ç­–ç•¥ï¼šé«˜å¬å›ç‡
        strategies['åŸºç¡€æœç´¢'] = [
            f'{drug_name}[Title/Abstract]',
            f'"{drug_name}"',
            f'{drug_name} AND review',
            f'{drug_name} AND clinical trial'
        ]

        # åŸºäºè¯ç‰©ç±»å‹çš„ä¸“é—¨ç­–ç•¥
        if drug_type != 'general' and confidence > 0.6:
            type_strategies = self._generate_type_specific_strategies(drug_name, drug_type)
            strategies.update(type_strategies)

        # æœºåˆ¶ç›¸å…³ç­–ç•¥
        strategies['æœºåˆ¶æ¢ç´¢'] = [
            f'{drug_name} AND (mechanism OR target OR action)',
            f'{drug_name} AND (inhibits OR activates OR binds)',
            f'{drug_name} AND (receptor OR enzyme OR protein)',
            f'{drug_name} AND pathway'
        ]

        # é«˜çº§ç§‘å­¦ç­–ç•¥
        strategies['ç§‘å­¦æ·±åº¦'] = [
            f'{drug_name} AND molecular',
            f'{drug_name} AND signaling',
            f'{drug_name} AND pharmacokinetics',
            f'{drug_name} AND pharmacodynamics',
            f'{drug_name} AND metabolism'
        ]

        # å®‰å…¨æ€§å’Œåº”ç”¨ç­–ç•¥
        strategies['ä¸´åºŠåº”ç”¨'] = [
            f'{drug_name} AND therapeutic',
            f'{drug_name} AND efficacy',
            f'{drug_name} AND safety',
            f'{drug_name} AND treatment',
            f'{drug_name} AND therapy'
        ]

        return strategies

    def _generate_type_specific_strategies(self, drug_name: str, drug_type: str) -> Dict[str, List[str]]:
        """ç”Ÿæˆç±»å‹ç‰¹å®šçš„æœç´¢ç­–ç•¥"""
        type_strategies = {}

        if drug_type == 'hormone':
            type_strategies['æ¿€ç´ è¯ç‰©'] = [
                f'{drug_name} AND hormone replacement',
                f'{drug_name} AND receptor agonist',
                f'{drug_name} AND endocrine',
                f'{drug_name} AND physiological',
                f'{drug_name} AND (thyroid OR estrogen OR testosterone)'
            ]
        elif drug_type == 'antibody':
            type_strategies['æŠ—ä½“è¯ç‰©'] = [
                f'{drug_name} AND monoclonal antibody',
                f'{drug_name} AND immunotherapy',
                f'{drug_name} AND antigen',
                f'{drug_name} AND (binding OR blockade)',
                f'{drug_name} AND immune checkpoint'
            ]
        elif drug_type == 'inhibitor':
            type_strategies['æŠ‘åˆ¶å‰‚'] = [
                f'{drug_name} AND inhibitor',
                f'{drug_name} AND inhibition',
                f'{drug_name} AND (enzyme OR kinase)',
                f'{drug_name} AND (blocks OR suppresses)',
                f'{drug_name} AND molecular target'
            ]
        elif drug_type == 'antibiotic':
            type_strategies['æŠ—ç”Ÿç´ '] = [
                f'{drug_name} AND antibiotic',
                f'{drug_name} AND antimicrobial',
                f'{drug_name} AND bacterial',
                f'{drug_name} AND (resistance OR susceptibility)',
                f'{drug_name} AND MIC'
            ]
        elif drug_type == 'antiviral':
            type_strategies['æŠ—ç—…æ¯’'] = [
                f'{drug_name} AND antiviral',
                f'{drug_name} AND virus',
                f'{drug_name} AND (viral inhibition OR viral replication)',
                f'{drug_name} AND (HIV OR hepatitis OR influenza)'
            ]

        return type_strategies

    def _execute_search_strategy_with_retry(self, drug_name: str, search_terms: List[str],
                                            max_needed: int) -> List[str]:
        """æ‰§è¡Œæœç´¢ç­–ç•¥ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        article_ids = []
        retry_count = 0
        max_retries = 2

        while retry_count <= max_retries and len(article_ids) < max_needed:
            if retry_count > 0:
                print(f"     ğŸ”„ é‡è¯• {retry_count}/{max_retries}")

            for i, search_term in enumerate(search_terms):
                if len(article_ids) >= max_needed:
                    break

                try:
                    # æ ¹æ®é‡è¯•æ¬¡æ•°è°ƒæ•´å‚æ•°
                    retry_delay = retry_count * 0.5
                    if retry_delay > 0:
                        time.sleep(retry_delay)

                    params = {
                        'db': 'pubmed',
                        'term': search_term,
                        'retmode': 'json',
                        'retmax': min(8, max_needed - len(article_ids)),
                        'sort': 'relevance',
                        'email': self.email,
                        'tool': self.tool
                    }

                    response = requests.get(f"{self.base_url}/esearch.fcgi", params=params, timeout=15)
                    response.raise_for_status()
                    data = response.json()
                    batch_ids = data.get('esearchresult', {}).get('idlist', [])

                    if batch_ids:
                        new_ids = [id for id in batch_ids if id not in article_ids]
                        if new_ids:
                            article_ids.extend(new_ids)
                            if retry_count == 0:
                                print(f"     âœ… æœç´¢è¯ {i + 1}: æ‰¾åˆ° {len(new_ids)} ç¯‡")
                            else:
                                print(f"     âœ… é‡è¯•æˆåŠŸ: æœç´¢è¯ {i + 1}: æ‰¾åˆ° {len(new_ids)} ç¯‡")
                    else:
                        if retry_count == 0:
                            print(f"     âš ï¸  æœç´¢è¯ {i + 1}: æ— ç»“æœ")

                except requests.exceptions.Timeout:
                    print(f"     â° æœç´¢è¯ {i + 1}: è¶…æ—¶")
                except requests.exceptions.ConnectionError:
                    print(f"     ğŸŒ æœç´¢è¯ {i + 1}: è¿æ¥é”™è¯¯")
                except Exception as e:
                    print(f"     âŒ æœç´¢è¯ {i + 1} å¤±è´¥: {str(e)[:50]}")

                # è¯·æ±‚é—´å»¶è¿Ÿ
                if i < len(search_terms) - 1:
                    time.sleep(0.2)

            # å¦‚æœæ‰¾åˆ°è¶³å¤Ÿæ–‡çŒ®ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
            if len(article_ids) >= max_needed:
                break

            retry_count += 1

        return article_ids

    def get_article_details(self, article_ids: List[str]) -> List[Dict]:
        """
        è·å–æ–‡ç« è¯¦ç»†ä¿¡æ¯
        """
        if not article_ids:
            return []

        # åˆ†æ‰¹å¤„ç†
        batch_size = min(5, len(article_ids))
        batches = [article_ids[i:i + batch_size] for i in range(0, len(article_ids), batch_size)]

        all_articles = []
        successful_batches = 0

        for i, batch in enumerate(batches):
            print(f"   ğŸ“„ æ‰¹æ¬¡ {i + 1}/{len(batches)}: è·å– {len(batch)} ç¯‡æ–‡çŒ®...")

            try:
                params = {
                    'db': 'pubmed',
                    'id': ','.join(batch),
                    'retmode': 'xml',
                    'retmax': len(batch)
                }

                response = requests.get(f"{self.base_url}/efetch.fcgi", params=params, timeout=30)
                response.raise_for_status()

                articles = self._parse_articles_xml(response.content)
                if articles:
                    all_articles.extend(articles)
                    successful_batches += 1
                    print(f"   âœ… æ‰¹æ¬¡æˆåŠŸ: è§£æ {len(articles)} ç¯‡æ–‡çŒ®")
                else:
                    print(f"   âš ï¸  æ‰¹æ¬¡è§£æå¤±è´¥")

            except Exception as e:
                print(f"   âŒ æ‰¹æ¬¡è·å–å¤±è´¥: {e}")

            # æ‰¹æ¬¡é—´å»¶è¿Ÿ
            if i < len(batches) - 1:
                time.sleep(1)

        print(f"   ğŸ“Š æ‰¹æ¬¡å¤„ç†å®Œæˆ: {successful_batches}/{len(batches)} ä¸ªæ‰¹æ¬¡æˆåŠŸ")
        return all_articles

    def _parse_articles_xml(self, xml_content: str) -> List[Dict]:
        """
        è§£æPubMed XML
        """
        try:
            root = ET.fromstring(xml_content)
            articles = []
            parsed_count = 0
            error_count = 0

            for article in root.findall('.//PubmedArticle'):
                try:
                    article_info = self._parse_single_article(article)
                    if article_info:
                        articles.append(article_info)
                        parsed_count += 1
                    else:
                        error_count += 1
                except Exception as e:
                    error_count += 1
                    continue

            print(f"   ğŸ“ XMLè§£æç»Ÿè®¡: {parsed_count} æˆåŠŸ, {error_count} å¤±è´¥")
            return articles

        except ET.ParseError as e:
            print(f"   âŒ XMLè§£æé”™è¯¯: {e}")
            return []
        except Exception as e:
            print(f"   âŒ è§£ææ–‡ç« XMLæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return []

    def _parse_single_article(self, article_element) -> Optional[Dict]:
        """
        è§£æå•ç¯‡æ–‡ç« 
        """
        try:
            # æå–PubMed ID
            pmid_element = article_element.find('.//PMID')
            pmid = pmid_element.text if pmid_element is not None else "Unknown"
            if pmid == "Unknown":
                return None

            # æå–æ ‡é¢˜
            title_element = article_element.find('.//ArticleTitle')
            title = title_element.text if title_element is not None else "No title"
            if not title or title == "No title" or len(title.strip()) < 5:
                return None

            # æå–æ‘˜è¦
            abstract_texts = []
            abstract_elements = article_element.findall('.//AbstractText')

            if not abstract_elements:
                abstract_elements = article_element.findall('.//Abstract/AbstractText')

            for abstract_element in abstract_elements:
                if abstract_element is not None and abstract_element.text:
                    text = abstract_element.text.strip()
                    if text and text not in abstract_texts:
                        abstract_texts.append(text)

            abstract = " ".join(abstract_texts) if abstract_texts else "No abstract available"

            # æå–å¹´ä»½
            year = self._extract_year(article_element)

            # æå–æœŸåˆŠä¿¡æ¯
            journal_element = article_element.find('.//Journal/Title')
            journal = journal_element.text if journal_element is not None else "Unknown"

            # æå–ä½œè€…ä¿¡æ¯ï¼ˆå‰3ä½ï¼‰
            authors = []
            author_elements = article_element.findall('.//AuthorList/Author')[:3]
            for author_elem in author_elements:
                last_name_elem = author_elem.find('LastName')
                fore_name_elem = author_elem.find('ForeName')
                if last_name_elem is not None and last_name_elem.text:
                    author_name = last_name_elem.text
                    if fore_name_elem is not None and fore_name_elem.text:
                        author_name += f" {fore_name_elem.text}"
                    authors.append(author_name)

            # æå–MeSHæœ¯è¯­ï¼ˆå…³é”®è¯ï¼‰
            mesh_terms = []
            mesh_elements = article_element.findall('.//MeshHeading/DescriptorName')[:5]
            for mesh_elem in mesh_elements:
                if mesh_elem is not None and mesh_elem.text:
                    mesh_terms.append(mesh_elem.text)

            return {
                'pubmed_id': pmid,
                'title': title,
                'abstract': abstract,
                'year': year,
                'journal': journal,
                'authors': authors[:3],
                'mesh_terms': mesh_terms[:5],
                'has_abstract': abstract != "No abstract available" and len(abstract) > 20
            }

        except Exception as e:
            return None

    def _extract_year(self, article_element) -> str:
        """
        å¢å¼ºçš„å¹´ä»½æå–æ–¹æ³•
        """
        # å°è¯•å¤šç§å¹´ä»½å­—æ®µ
        year_sources = [
            ('.//PubDate/Year', 'æ ‡å‡†å¹´ä»½'),
            ('.//ArticleDate/Year', 'æ–‡ç« æ—¥æœŸ'),
            ('.//MedlineDate', 'Medlineæ—¥æœŸ'),
            ('.//PubMedPubDate[@PubStatus="pubmed"]/Year', 'PubMedæ—¥æœŸ'),
            ('.//PubMedPubDate[@PubStatus="medline"]/Year', 'Medlineç´¢å¼•æ—¥æœŸ')
        ]

        for xpath, source_name in year_sources:
            element = article_element.find(xpath)
            if element is not None and element.text:
                text = element.text.strip()
                if text:
                    year_match = re.search(r'(\d{4})', text)
                    if year_match:
                        year = year_match.group(1)
                        if 1800 <= int(year) <= 2100:
                            return year

        return "Unknown"

    def _is_english_abstract(self, text: str) -> bool:
        """
        ä¼˜åŒ–çš„è‹±æ–‡æ£€æµ‹æ–¹æ³•
        """
        if not text or len(text) < 20:
            return False

        # 1. æ£€æµ‹ä¸­æ–‡å­—ç¬¦ï¼ˆä¸¥æ ¼æ’é™¤ï¼‰
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        if chinese_chars > 0:
            return False

        # 2. æ£€æµ‹éæ‹‰ä¸å­—æ¯ï¼ˆå®½æ¾ï¼‰
        latin_chars = sum(
            1 for char in text if ('\u0041' <= char <= '\u005a') or ('\u0061' <= char <= '\u007a') or char.isspace())
        latin_ratio = latin_chars / len(text) if len(text) > 0 else 0

        if latin_ratio < 0.6:
            return False

        # 3. ç®€å•è‹±æ–‡è¯æ±‡æ£€æµ‹
        words = re.findall(r'\b[a-zA-Z]{2,}\b', text)
        if len(words) < 8:
            return False

        # 4. å¸¸è§çš„è‹±æ–‡è¯æ±‡ï¼ˆæ ¸å¿ƒè¯æ±‡ï¼‰
        english_core_words = {'the', 'and', 'of', 'in', 'to', 'a', 'is', 'that', 'for', 'on', 'was', 'with', 'as', 'by',
                              'be'}

        words_lower = [w.lower() for w in words]
        english_count = sum(1 for word in words_lower if word in english_core_words)
        english_ratio = english_count / len(words) if len(words) > 0 else 0

        return english_ratio > 0.05

    def _detect_language(self, text: str) -> str:
        """
        æ£€æµ‹æ–‡æœ¬è¯­è¨€
        """
        if not text:
            return 'unknown'

        # æ£€æµ‹ä¸­æ–‡å­—ç¬¦
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        if chinese_chars > 0:
            chinese_ratio = chinese_chars / len(text)
            if chinese_ratio > 0.3:
                return 'chinese'
            else:
                return 'mixed'

        # æ£€æµ‹è‹±æ–‡
        if self._is_english_abstract(text):
            return 'english'

        # æ£€æµ‹å…¶ä»–æ‹‰ä¸å­—æ¯è¯­è¨€
        latin_chars = sum(1 for char in text if ('\u0041' <= char <= '\u005a') or ('\u0061' <= char <= '\u007a'))
        latin_ratio = latin_chars / len(text) if len(text) > 0 else 0

        if latin_ratio > 0.7:
            return 'latin_other'
        else:
            return 'other'

    def search_drug_articles(self, drug_name: str, max_results: int = None) -> List[Dict]:
        """å…¼å®¹åŸæœ‰æ¥å£ï¼Œè°ƒç”¨æ™ºèƒ½æœç´¢"""
        return self.smart_search_drug_articles(drug_name, max_results)