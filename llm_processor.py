import requests
import json
import re
from typing import List, Dict, Optional
import time
import numpy as np
import os
import csv
from datetime import datetime
import pandas as pd


class LLMProcessor:
    def __init__(self, config: Dict):
        self.api_key = config['zhipu']['ZHIPUAI_API_KEY']
        self.base_url = config['zhipu']['base_url']
        self.model = config['zhipu']['model']
        self.temperature = config['zhipu']['temperature']
        self.max_tokens = config['zhipu']['max_tokens']

        # æ·»åŠ tokenç»Ÿè®¡
        self.token_usage_file = 'data/output/token_usage.csv'
        self._ensure_token_file()

    def _ensure_token_file(self):
        """ç¡®ä¿tokenç»Ÿè®¡æ–‡ä»¶å­˜åœ¨"""
        os.makedirs(os.path.dirname(self.token_usage_file), exist_ok=True)
        if not os.path.exists(self.token_usage_file):
            with open(self.token_usage_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['timestamp', 'drug_name', 'article_id', 'prompt_tokens',
                                 'completion_tokens', 'total_tokens', 'cost_estimate'])

    def _record_token_usage(self, drug_name: str, article_id: str, response_data: Dict):
        """è®°å½•tokenä½¿ç”¨æƒ…å†µ"""
        try:
            usage = response_data.get('usage', {})
            prompt_tokens = usage.get('prompt_tokens', 0)
            completion_tokens = usage.get('completion_tokens', 0)
            total_tokens = usage.get('total_tokens', 0)

            cost_estimate = (prompt_tokens * 0.1 / 1000) + (completion_tokens * 0.1 / 1000)

            with open(self.token_usage_file, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([
                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    drug_name,
                    article_id,
                    prompt_tokens,
                    completion_tokens,
                    total_tokens,
                    round(cost_estimate, 4)
                ])

            print(
                f"ğŸ’° Tokenä½¿ç”¨: è¾“å…¥{prompt_tokens}, è¾“å‡º{completion_tokens}, æ€»è®¡{total_tokens}, ä¼°ç®—æˆæœ¬Â¥{cost_estimate:.4f}")

        except Exception as e:
            print(f"âš ï¸ è®°å½•tokenä½¿ç”¨å¤±è´¥: {e}")

    def analyze_article(self, drug_name: str, article: Dict, article_id: str) -> Optional[Dict]:
        """
        ä½¿ç”¨æ™ºè°±AIåˆ†æå•ç¯‡æ–‡ç« ï¼Œæå–è¯ç‰©-é¶ç‚¹å…³ç³»
        """
        prompt = self._build_prompt(drug_name, article, article_id)

        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }

        data = {
            'model': self.model,
            'messages': [
                {
                    'role': 'system',
                    'content': "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”Ÿç‰©åŒ»å­¦ç ”ç©¶åŠ©æ‰‹ï¼Œä¸“é—¨ä»åŒ»å­¦æ–‡çŒ®ä¸­æå–è¯ç‰©-é¶ç‚¹å…³ç³»ä¿¡æ¯ã€‚è¯·æ ¹æ®è¯æ®å¼ºåº¦è®¾ç½®ç½®ä¿¡ç­‰çº§ï¼Œå¹¶ç¡®ä¿è¾“å‡ºæ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚"
                },
                {
                    'role': 'user',
                    'content': prompt
                }
            ],
            'temperature': self.temperature,
            'max_tokens': self.max_tokens
        }

        try:
            print(f"å‘é€è¯·æ±‚åˆ°æ™ºè°±AI API...")
            response = requests.post(
                f"{self.base_url}chat/completions",
                headers=headers,
                data=json.dumps(data, ensure_ascii=False),
                timeout=60
            )
            response.raise_for_status()

            result = response.json()

            self._record_token_usage(drug_name, article_id, result)

            content = result['choices'][0]['message']['content']

            print(f"LLMåˆ†æå®Œæˆï¼Œå“åº”é•¿åº¦: {len(content)} å­—ç¬¦")

            return self._parse_llm_response(content, article, article_id)

        except requests.exceptions.Timeout:
            print("æ™ºè°±AI APIè°ƒç”¨è¶…æ—¶")
            return None
        except Exception as e:
            print(f"æ™ºè°±AI APIè°ƒç”¨é”™è¯¯: {e}")
            if hasattr(e, 'response'):
                print(f"å“åº”å†…å®¹: {e.response.text}")
            return None

    def _build_prompt(self, drug_name: str, article: Dict, article_id: str) -> str:
        """
        æ„å»ºåˆ†ææç¤ºè¯
        """
        abstract = article['abstract']
        if len(abstract) > 3000:
            abstract = abstract[:3000] + "... [æ‘˜è¦è¢«æˆªæ–­]"

        return f"""è¯·ä»ä»¥ä¸‹åŒ»å­¦æ–‡çŒ®ä¸­ç²¾ç¡®æå–è¯ç‰©-é¶ç‚¹å…³ç³»ä¿¡æ¯ï¼š

è¯ç‰©åç§°ï¼š{drug_name}
æ–‡çŒ®ç¼–å·ï¼š{article_id}
æ–‡çŒ®æ ‡é¢˜ï¼š{article['title']}
æ–‡çŒ®æ‘˜è¦ï¼š{abstract}
å‘è¡¨å¹´ä»½ï¼š{article['year']}
PubMed IDï¼š{article['pubmed_id']}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š

{{
    "drug_name": "{drug_name}",
    "article_id": "{article_id}",
    "targets": [
        {{
            "target_name": "å…·ä½“é¶ç‚¹è›‹ç™½åç§°",
            "genes": ["ç›¸å…³åŸºå› 1", "ç›¸å…³åŸºå› 2"],
            "pathways": ["ç›¸å…³ä¿¡å·é€šè·¯1", "ç›¸å…³ä¿¡å·é€šè·¯2"],
            "reference": "ä»æ‘˜è¦ä¸­ç›´æ¥å¤åˆ¶æ”¯æŒè¯¥å…³ç³»çš„å…·ä½“å¥å­",
            "confidence_level": "high/medium/low"
        }}
    ],
    "title": "{article['title']}",
    "year": "{article['year']}",
    "pubmed_id": "{article['pubmed_id']}"
}}

ç½®ä¿¡ç­‰çº§è¯´æ˜ï¼š
- high: æ–‡çŒ®æ˜ç¡®ç›´æ¥æåˆ°è¯¥é¶ç‚¹ï¼Œæœ‰å®éªŒè¯æ®æ”¯æŒï¼Œå¦‚"inhibits", "binds to", "targets"ç­‰æ˜ç¡®è¯æ±‡
- medium: æ–‡çŒ®é—´æ¥æåˆ°æˆ–åŸºäºå·²çŸ¥æœºåˆ¶çš„æ¨æ–­ï¼Œå¦‚"associated with", "involved in", "related to"ç­‰
- low: åŸºäºç›¸å…³æ€§çš„æ¨æµ‹ï¼Œè¯æ®è¾ƒå¼±ï¼Œæˆ–éœ€è¦è¿›ä¸€æ­¥éªŒè¯çš„å…³ç³»

æå–è§„åˆ™ï¼š
1. æ¯ä¸ªæ˜ç¡®æåˆ°çš„é¶ç‚¹å•ç‹¬ä¸€ä¸ªå¯¹è±¡ï¼Œæ¯ä¸ªé¶ç‚¹å ä¸€è¡Œ
2. åªæå–æ–‡çŒ®ä¸­æ˜ç¡®æåˆ°çš„ä¿¡æ¯ï¼Œä¸è¦è¿‡åº¦æ¨æ–­
3. åŸºå› åç§°ä½¿ç”¨æ ‡å‡†ç¬¦å·ï¼ˆå¦‚ STAT3, EGFR, TP53ï¼‰
4. é¶ç‚¹åç§°è¦å…·ä½“æ˜ç¡®ï¼ˆå¦‚ "cyclooxygenase-1" è€Œä¸æ˜¯ "COX"ï¼‰
5. é€šè·¯åç§°è¦å®Œæ•´ï¼ˆå¦‚ "PI3K/AKT signaling pathway"ï¼‰
6. å¼•ç”¨å¥å­å¿…é¡»æ¥è‡ªåŸæ–‡æ‘˜è¦ï¼Œç›´æ¥å¤åˆ¶åŸæ–‡
7. æ ¹æ®è¯æ®å¼ºåº¦åˆç†è®¾ç½®ç½®ä¿¡ç­‰çº§
8. å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…³ç³»ï¼Œtargetsè®¾ä¸ºç©ºæ•°ç»„[]
9. ç¡®ä¿è¾“å‡ºæ˜¯çº¯JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡æœ¬

é‡è¦ï¼šè¯·ä»”ç»†è¯„ä¼°è¯æ®å¼ºåº¦ï¼Œåˆç†è®¾ç½®ç½®ä¿¡ç­‰çº§ï¼

ç°åœ¨è¯·åˆ†ææ–‡çŒ®å¹¶è¾“å‡ºJSONï¼š"""

    def _parse_llm_response(self, content: str, article: Dict, article_id: str) -> Optional[Dict]:
        """
        è§£æLLMçš„å“åº”
        """
        try:
            print("å¼€å§‹è§£æLLMå“åº”...")

            content = content.strip()
            print(f"åŸå§‹å“åº”é•¿åº¦: {len(content)}")

            json_content = self._extract_json(content)
            if not json_content:
                print("æ— æ³•ä»å“åº”ä¸­æå–JSONå†…å®¹")
                return None

            print(f"æå–çš„JSONå†…å®¹: {json_content[:100]}...")

            result = json.loads(json_content)

            result = self._validate_and_fix_fields(result, article, article_id)

            if self._has_valid_content(result):
                print(f"âœ… æˆåŠŸæå–: {len(result['targets'])} ä¸ªé¶ç‚¹")
                confidence_counts = {}
                for target in result['targets']:
                    conf_level = target.get('confidence_level', 'medium')
                    confidence_counts[conf_level] = confidence_counts.get(conf_level, 0) + 1
                print(f"ğŸ¯ ç½®ä¿¡ç­‰çº§åˆ†å¸ƒ: {confidence_counts}")
                return result
            else:
                print("âŒ æœªæå–åˆ°æœ‰æ•ˆçš„é¶ç‚¹ä¿¡æ¯")
                return None

        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æé”™è¯¯: {e}")
            print(f"è§£æå†…å®¹: {content[:200]}...")
            return None
        except Exception as e:
            print(f"âŒ å¤„ç†LLMå“åº”æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None

    def _extract_json(self, content: str) -> Optional[str]:
        """ä»å“åº”å†…å®¹ä¸­æå–JSONéƒ¨åˆ†"""
        json_patterns = [
            r'```json\s*(.*?)\s*```',
            r'```\s*(.*?)\s*```',
            r'\{.*\}'
        ]

        for pattern in json_patterns:
            matches = re.findall(pattern, content, re.DOTALL)
            if matches:
                extracted = matches[0].strip()
                if extracted.startswith('{') and extracted.endswith('}'):
                    print(f"ä½¿ç”¨æ¨¡å¼æ‰¾åˆ°JSON: {pattern}")
                    return extracted

        try:
            start = content.find('{')
            end = content.rfind('}') + 1
            if start != -1 and end != -1 and start < end:
                potential_json = content[start:end]
                json.loads(potential_json)
                print("ç›´æ¥æå–JSONæˆåŠŸ")
                return potential_json
        except:
            pass

        if content.startswith('{') and content.endswith('}'):
            print("ä½¿ç”¨æ•´ä¸ªå†…å®¹ä½œä¸ºJSON")
            return content

        return None

    def _validate_and_fix_fields(self, result: Dict, article: Dict, article_id: str) -> Dict:
        """éªŒè¯å’Œä¿®å¤å­—æ®µ"""
        default_structure = {
            'drug_name': '',
            'article_id': article_id,
            'targets': [],
            'title': article['title'],
            'year': article['year'],
            'pubmed_id': article['pubmed_id']
        }

        for key, default_value in default_structure.items():
            if key not in result:
                result[key] = default_value
                print(f"æ·»åŠ ç¼ºå¤±å­—æ®µ: {key} = {default_value}")

        result['drug_name'] = str(result.get('drug_name', ''))
        result['article_id'] = str(result.get('article_id', article_id))

        if not isinstance(result['targets'], list):
            result['targets'] = []

        valid_targets = []
        for target in result['targets']:
            if isinstance(target, dict) and target.get('target_name'):
                target.setdefault('genes', [])
                target.setdefault('pathways', [])
                target.setdefault('reference', '')
                target.setdefault('confidence_level', 'medium')

                if not isinstance(target['genes'], list):
                    if isinstance(target['genes'], str):
                        target['genes'] = [gene.strip() for gene in target['genes'].split(',') if gene.strip()]
                    else:
                        target['genes'] = []

                if not isinstance(target['pathways'], list):
                    if isinstance(target['pathways'], str):
                        target['pathways'] = [pathway.strip() for pathway in target['pathways'].split(',') if
                                              pathway.strip()]
                    else:
                        target['pathways'] = []

                if target['confidence_level'] not in ['high', 'medium', 'low']:
                    print(f"âš ï¸  æ— æ•ˆçš„ç½®ä¿¡ç­‰çº§ '{target['confidence_level']}'ï¼Œè®¾ç½®ä¸ºé»˜è®¤å€¼ 'medium'")
                    target['confidence_level'] = 'medium'

                target['genes'] = [gene for gene in target['genes'] if gene and str(gene).strip()]
                target['pathways'] = [pathway for pathway in target['pathways'] if pathway and str(pathway).strip()]
                target['reference'] = str(target.get('reference', '')).strip()

                valid_targets.append(target)
                print(f"   âœ… é¶ç‚¹ '{target['target_name']}' - ç½®ä¿¡ç­‰çº§: {target['confidence_level']}")

        result['targets'] = valid_targets

        return result

    def _has_valid_content(self, result: Dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆå†…å®¹"""
        return len(result.get('targets', [])) > 0

    def batch_analyze_articles(self, drug_name: str, drug_index: int, articles: List[Dict]) -> List[Dict]:
        """
        æ‰¹é‡åˆ†æå¤šç¯‡æ–‡ç« 
        """
        results = []

        for i, article in enumerate(articles, 1):
            if article is None:
                print(f"âš ï¸  è·³è¿‡ç¬¬ {i} ç¯‡æ–‡çŒ®ï¼šæ–‡ç« ä¿¡æ¯ä¸ºç©º")
                continue

            article_id = f"D{drug_index:03d}.P{i:02d}"
            print(f"\nğŸ“– åˆ†æç¬¬ {i}/{len(articles)} ç¯‡æ–‡çŒ® (ç¼–å·: {article_id}):")
            print(f"   æ ‡é¢˜: {article['title'][:80]}...")
            print(f"   å¹´ä»½: {article['year']}")

            analysis_result = self.analyze_article(drug_name, article, article_id)

            if analysis_result:
                results.append(analysis_result)
                confidence_levels = [t.get('confidence_level', 'medium') for t in analysis_result['targets']]
                print(f"   âœ… æå–æˆåŠŸï¼Œæ‰¾åˆ° {len(analysis_result['targets'])} ä¸ªé¶ç‚¹")
                print(f"   ğŸ¯ ç½®ä¿¡ç­‰çº§: {dict(zip(*np.unique(confidence_levels, return_counts=True)))}")
            else:
                print(f"   âŒ æå–å¤±è´¥æˆ–æ— æœ‰æ•ˆå…³ç³»")

            time.sleep(1.5)

        print(f"\nğŸ¯ è¯ç‰© {drug_name} åˆ†æå®Œæˆ: {len(results)}/{len(articles)} ç¯‡æ–‡çŒ®æå–åˆ°å…³ç³»")

        if results:
            all_confidence = []
            for result in results:
                for target in result['targets']:
                    all_confidence.append(target.get('confidence_level', 'medium'))

            from collections import Counter
            confidence_stats = Counter(all_confidence)
            print(f"ğŸ“Š æ€»ä½“ç½®ä¿¡ç­‰çº§åˆ†å¸ƒ: {dict(confidence_stats)}")

        return results